# -*- coding: utf-8 -*-
"""smile_nonsmile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m4YG4QqLRreLxGjqhYD5wj19OXPI27_w
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("talhasar/genki4k")

print("Path to dataset files:", path)

!pip install tf-keras
!pip install retina-face

"""
giving each image of the dataset to retina face, detecting faces and cropp them.
the cropped images is saved into the output path in 2 folders(smile and non_smile).

"""
import os
import cv2
from retinaface import RetinaFace


input_path = path + '/kaggle-genki4k'
output_path = '/root/.cache/kagglehub/datasets/cropped_genki4k'


classes = ['smile', 'non_smile']
fixed_size = (128, 128)

for class_name in classes:
    os.makedirs(os.path.join(output_path, class_name), exist_ok=True)

for class_name in classes:
    class_input_path = os.path.join(input_path, class_name)
    class_output_path = os.path.join(output_path, class_name)

    for image_name in os.listdir(class_input_path):
        image_path = os.path.join(class_input_path, image_name)

        image = cv2.imread(image_path)
        if image is None:
            print(f"Could not read image {image_path}. Skipping.")
            continue

        # Detect faces in the image
        faces = RetinaFace.detect_faces(image_path)

        if isinstance(faces, dict):
            for i, key in enumerate(faces.keys()):
                # Get face coordinates and crop the image
                face = faces[key]
                facial_area = face['facial_area']
                x1, y1, x2, y2 = facial_area

                # Crop the face from the image
                cropped_face = image[y1:y2, x1:x2]
                cropped_face = cv2.resize(cropped_face, fixed_size)

                # Save the cropped face to the new directory
                output_image_path = os.path.join(class_output_path, f"{os.path.splitext(image_name)[0]}_face_{i}.jpg")
                cv2.imwrite(output_image_path, cropped_face)

                print(f"Saved cropped face to {output_image_path}")
        else:
            print(f"No face detected in {image_path}.")

"""
show some images from cropped ones

"""
import os
import cv2
import matplotlib.pyplot as plt

# Number of images to display
num_images = 10

# Display images from each class
for class_name in classes:
    class_path = os.path.join(output_path, class_name)
    image_files = os.listdir(class_path)

    # Select a subset of images to display
    image_files = image_files[:num_images]

    # Plot the images
    plt.figure(figsize=(20, 10))
    plt.suptitle(f"Sample images from '{class_name}' class", fontsize=16)

    for i, image_name in enumerate(image_files):
        image_path = os.path.join(class_path, image_name)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct color display

        # Display the image in a subplot
        plt.subplot(2, num_images // 2, i + 1)
        plt.imshow(image)
        plt.axis('off')
        plt.title(f"Image {i + 1}")

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

!pip install face-alignment
!pip install torch

"""
align cropped images.

"""
import os
import cv2
import torch
import numpy as np
import face_alignment
from matplotlib import pyplot as plt

# Initialize the face alignment module
fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device='cuda' if torch.cuda.is_available() else 'cpu')

#output_path = '/root/.cache/kagglehub/datasets/cropped_genki4k'
output_aligned_path = '/root/.cache/kagglehub/datasets/aligned_cropped_genki4k'
classes = ['smile', 'non_smile']

# Create output directories for aligned images if they don't exist
for class_name in classes:
    os.makedirs(os.path.join(output_aligned_path, class_name), exist_ok=True)

# Function to align face using eye landmarks
def align_face(image, landmarks, target_size=(128, 128)):
    # Select coordinates for left and right eyes (landmarks 36-39 for left, 42-45 for right)
    left_eye = np.mean(landmarks[36:42], axis=0)
    right_eye = np.mean(landmarks[42:48], axis=0)

    # Calculate the angle between the eye centroids
    delta_x = right_eye[0] - left_eye[0]
    delta_y = right_eye[1] - left_eye[1]
    angle = np.degrees(np.arctan2(delta_y, delta_x))

    # Calculate center between the eyes
    eye_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)

    # Get the rotation matrix for rotating the image around the eye center
    M = cv2.getRotationMatrix2D(eye_center, angle, scale=1.0)

    # Perform the rotation
    aligned_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]), flags=cv2.INTER_CUBIC)

    # Resize the aligned image to the target size
    aligned_image_resized = cv2.resize(aligned_image, target_size)

    return aligned_image_resized

# Iterate through each class and its images
for class_name in classes:
    class_input_path = os.path.join(output_path, class_name)
    class_output_path = os.path.join(output_aligned_path, class_name)

    for image_name in os.listdir(class_input_path):
        image_path = os.path.join(class_input_path, image_name)

        # Read the image
        image = cv2.imread(image_path)
        if image is None:
            print(f"Could not read image {image_path}. Skipping.")
            continue

        # Convert the image to RGB for the face alignment library
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Get landmarks for face alignment
        landmarks = fa.get_landmarks(image_rgb)

        if landmarks:
            # Align the image using the landmarks
            aligned_face = align_face(image_rgb, landmarks[0])

            # Convert aligned face back to BGR for saving with OpenCV
            aligned_face_bgr = cv2.cvtColor(aligned_face, cv2.COLOR_RGB2BGR)

            # Save the aligned image
            output_image_path = os.path.join(class_output_path, image_name)
            cv2.imwrite(output_image_path, aligned_face_bgr)

            print(f"Aligned image saved to {output_image_path}")
        else:
            print(f"No landmarks detected in {image_path}. Skipping.")

import os
import cv2
import matplotlib.pyplot as plt

# Path to the aligned dataset
classes = ['smile', 'non_smile']

# Number of images to display
num_images = 10  # You can change this to display more or fewer images

# Display images from each class
for class_name in classes:
    class_path = os.path.join(output_aligned_path, class_name)
    image_files = os.listdir(class_path)[:num_images]  # Get the first `num_images` images

    # Plot the images
    plt.figure(figsize=(15, 5))
    plt.suptitle(f"Aligned images from '{class_name}' class", fontsize=16)

    for i, image_name in enumerate(image_files):
        image_path = os.path.join(class_path, image_name)
        image = cv2.imread(image_path)
        if image is None:
            print(f"Could not read image {image_path}. Skipping.")
            continue


        # Convert BGR to RGB for correct color display
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Display the image in a subplot
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image_rgb)
        plt.axis('off')
        plt.title(f"Image {i + 1}")

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

"""
download the .hdf5 file of the pretrained model

"""
import locale
def set_utf8_locale():
    # Try setting the locale to en_US.UTF-8.
    try:
        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
        return True
    except locale.Error:
        # If en_US.UTF-8 is not available, try other UTF-8 locales.
        for loc in ['en_US.utf8', 'C.UTF-8']:
            try:
                locale.setlocale(locale.LC_ALL, loc)
                return True
            except locale.Error:
                pass
        return False  # Return False if no suitable locale was found.

if set_utf8_locale():
    print("Locale set to UTF-8 successfully.")
else:
    print("Warning: Could not set locale to UTF-8. Some functionality may be affected.")

import os
# Set locale to UTF-8 using environment variables
os.environ['LC_ALL'] = 'en_US.UTF-8'
os.environ['LANG'] = 'en_US.UTF-8'

# Download the HDF5 file using the os module
os.system('wget -O file.hdf5 https://github.com/meng1994412/Smile_Detection/raw/master/output/lenet.hdf5')

import tensorflow as tf
from tensorflow.keras import layers, models


# Load the model from the .hdf5 file
model = tf.keras.models.load_model('file.hdf5')

# Print the model summary to understand its architecture
model.summary()

for layer in model.layers:
    layer.trainable = True

x = model.layers[-2].output
x = layers.Dense(32, activation='relu', name='custom_dense1')(x)
x = layers.Dropout(0.5, name='custom_drop1')(x)
x = layers.Dense(32, activation='relu', name='custom_dense2')(x)
x = layers.Dense(16, activation='relu', name='custom_dense3')(x)
x = layers.Dropout(0.5, name='custom_drop2')(x)
x = layers.Dense(16, activation='relu', name='custom_dense4')(x)
x = layers.Dense(4, activation='relu', name='custom_dense5')(x)
output = layers.Dense(1, activation='sigmoid', name='custom_dense6')(x)

model = models.Model(inputs=model.input, outputs=output)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy'])

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dir = output_aligned_path

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    #zoom_range=0.2,
    #horizontal_flip=True,
    #fill_mode='nearest'
)

# Create data loaders
train_generator = datagen.flow_from_directory(
    train_dir,
    target_size=(28, 28),
    batch_size=25,
    class_mode='binary',
    color_mode='grayscale',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    train_dir,
    target_size=(28, 28),
    batch_size=25,
    class_mode='binary',
    color_mode='grayscale',
    subset='validation'
)

batch = next(train_generator)
print("Input data shape:", batch[0].shape)
print("Label shape:", batch[1].shape)

for i, layer in enumerate(model.layers):
    print(f"Layer {i}: {layer.name}")

import os

os.environ["CUDA_VISIBLE_DEVICES"] = "1"
import tensorflow as tf

for i, layer in enumerate(model.layers):
    print(f"Layer {i}: {layer.name} - Input shape: {layer.input_shape}, Output shape: {layer.output_shape}")

#for checking the val_loss
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=2,
    verbose=1,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0)

history_fine = model.fit(
    train_generator,
    validation_data=validation_generator,
    #batch_size=128,
    epochs=25,
    #verbose=1,
    steps_per_epoch=len(train_generator),
    validation_steps=len(validation_generator),
    callbacks=[early_stopping])

from google.colab import drive
drive.mount('/content/drive')

# Save the model
model.save('retrained_smile_nonsmile.h5')
print("Model saved as 'retrained_smile_nonsmile.h5'")

# Evaluate the model
loss, accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {accuracy:.2%}')

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!cp retrained_smile_nonsmile.h5 /content/drive/MyDrive

"""
plotting the metrics

"""
import matplotlib.pyplot as plt


plt.figure(figsize=(12, 6))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history_fine.history['accuracy'], label='Training Accuracy')
plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='best')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history_fine.history['loss'], label='Training Loss')
plt.plot(history_fine.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='best')

plt.tight_layout()
plt.show()

"""
download the mp4 file for testing

"""
import locale
def set_utf8_locale():
    # Try setting the locale to en_US.UTF-8.
    try:
        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
        return True
    except locale.Error:
        # If en_US.UTF-8 is not available, try other UTF-8 locales.
        for loc in ['en_US.utf8', 'C.UTF-8']:
            try:
                locale.setlocale(locale.LC_ALL, loc)
                return True
            except locale.Error:
                pass
        return False  # Return False if no suitable locale was found.

if set_utf8_locale():
    print("Locale set to UTF-8 successfully.")
else:
    print("Warning: Could not set locale to UTF-8. Some functionality may be affected.")

import os
# Set locale to UTF-8 using environment variables
os.environ['LC_ALL'] = 'en_US.UTF-8'
os.environ['LANG'] = 'en_US.UTF-8'

# Download the mp4 file using the os module
os.system('wget -O test.mp4 https://youtu.be/nA1dXR7im7o')

"""
test section, testing the model using a short video.

"""

import cv2
import numpy as np
from tensorflow.keras.models import load_model
from retinaface import RetinaFace

# Load the pre-trained model
model = load_model('retrained_smile_nonsmile.h5')

# Open the video file
video_path = '/content/People Smiling (No sound).mp4'
cap = cv2.VideoCapture(video_path)

# Get video properties
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

# Define the codec and create VideoWriter object
output_path = 'output_video_with_predictions.avi'
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Process the video frame by frame
frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break  # Exit loop when the video ends

    # Detect faces using RetinaFace
    detections = RetinaFace.detect_faces(frame)

    if isinstance(detections, dict):
        for key in detections:
            face_info = detections[key]
            facial_area = face_info['facial_area']
            x1, y1, x2, y2 = facial_area

            # Crop the detected face
            face_crop = frame[y1:y2, x1:x2]

            # Resize the cropped face to 128x128
            resized_face = cv2.resize(face_crop, (128, 128))
            resized_face = cv2.resize(face_crop, (28, 28))
            # Convert to grayscale
            gray_face = cv2.cvtColor(resized_face, cv2.COLOR_BGR2GRAY)

            # Normalize and reshape for the model
            normalized_face = gray_face / 255.0
            input_face = np.expand_dims(normalized_face, axis=(0, -1))  # Shape: (1, 28, 28, 1)

            # Predict using the model
            prediction = model.predict(input_face)
            label = 'Smile' if prediction[0] > 0.5 else 'No Smile'

            # Overlay the prediction text on the frame
            font = cv2.FONT_HERSHEY_SIMPLEX
            text_color = (0, 255, 0) if label == 'Smile' else (0, 0, 255)  # Green for smile, red for no smile
            cv2.putText(frame, f'Prediction: {label}', (x1, y1 - 10), font, 1, text_color, 2, cv2.LINE_AA)

            cv2.rectangle(frame, (x1, y1), (x2, y2), text_color, 2)

    # Write the frame to the output video
    out.write(frame)

    # Print progress
    frame_count += 1
    print(f'Processing frame {frame_count}/{total_frames}', end='\r')

cap.release()
out.release()
cv2.destroyAllWindows()

print("\nVideo processing complete. Output saved to:", output_path)


import locale
locale.getpreferredencoding = lambda: "UTF-8"

!cp output_video_with_predictions.avi /content/drive/MyDrive